{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bda66464bcb344f09533a19a6eec2201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b0a54732562a4dc088be68f00876bc53",
              "IPY_MODEL_4e4081e808534d46a97701da9f272025",
              "IPY_MODEL_af8e0df5f86f44fbbf2ab13e5ae19245"
            ],
            "layout": "IPY_MODEL_2082e738c0c04e29a5e46488aa418267"
          }
        },
        "b0a54732562a4dc088be68f00876bc53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_221ceca54649428f8f8a96b96e971550",
            "placeholder": "​",
            "style": "IPY_MODEL_cda1c78ae8b24849bb01e2520c8ec612",
            "value": "  0%"
          }
        },
        "4e4081e808534d46a97701da9f272025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ea5bb22f13d49fca038318bc3d9e760",
            "max": 140,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ce50966e61c4d46934346d679a8b05c",
            "value": 0
          }
        },
        "af8e0df5f86f44fbbf2ab13e5ae19245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_805c29166cbd4c9289b80b4f87d1819a",
            "placeholder": "​",
            "style": "IPY_MODEL_988c3cc96f9443f78a23c5831bf6307b",
            "value": " 0/140 [00:00&lt;?, ?it/s]"
          }
        },
        "2082e738c0c04e29a5e46488aa418267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "221ceca54649428f8f8a96b96e971550": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cda1c78ae8b24849bb01e2520c8ec612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ea5bb22f13d49fca038318bc3d9e760": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ce50966e61c4d46934346d679a8b05c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "805c29166cbd4c9289b80b4f87d1819a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "988c3cc96f9443f78a23c5831bf6307b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akalpit23/Adversarial-Patch/blob/main/Adversarial_Patch_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AIPI 590 - XAI | Assignment #02\n",
        "\n",
        "### Akalpit Dawkhar\n"
      ],
      "metadata": {
        "id": "trLU6lavfYEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision adversarial-robustness-toolbox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPiIhFOSbrnl",
        "outputId": "90fbab30-f670-43f9-ae0b-f1ee2af0cc15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Collecting adversarial-robustness-toolbox\n",
            "  Downloading adversarial_robustness_toolbox-1.18.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (71.0.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (4.66.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading adversarial_robustness_toolbox-1.18.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: adversarial-robustness-toolbox\n",
            "Successfully installed adversarial-robustness-toolbox-1.18.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDFHTE60fTUv",
        "outputId": "c6c15d91-7831-4bba-9b08-b19632f8e1a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Adversarial-Patch'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 48 (delta 16), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (48/48), 33.23 KiB | 773.00 KiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n",
            "[Errno 20] Not a directory: 'Adversarial-Patch/Adversarial_Patch_Assignment_02.ipynb'\n",
            "/content\n",
            "\u001b[0m\u001b[01;34mAdversarial-Patch\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "# Please use this to connect your GitHub repository to your Google Colab notebook\n",
        "# Connects to any needed files from GitHub and Google Drive\n",
        "import os\n",
        "\n",
        "# Remove Colab default sample_data\n",
        "!rm -r ./sample_data\n",
        "\n",
        "# Clone GitHub files to colab workspace\n",
        "repo_name = \"Adversarial-Patch\" # Change to your repo name\n",
        "git_path = 'https://github.com/akalpit23/Adversarial-Patch.git' #Change to your path\n",
        "!git clone \"{git_path}\"\n",
        "\n",
        "# Install dependencies from requirements.txt file\n",
        "#!pip install -r \"{os.path.join(repo_name,'requirements.txt')}\" #Add if using requirements.txt\n",
        "\n",
        "# Change working directory to location of notebook\n",
        "notebook_dir = 'Adversarial_Patch_Assignment_02.ipynb'\n",
        "path_to_notebook = os.path.join(repo_name,notebook_dir)\n",
        "%cd \"{path_to_notebook}\"\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import scipy.linalg\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "## Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial10\"\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Fetching the device that will be used throughout this notebook\n",
        "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
        "print(\"Using device\", device)"
      ],
      "metadata": {
        "id": "yDWvXe83bx_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd56504-cb63-4855-93a0-958ce8a1b166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-f3d2f58672aa>:14: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  set_matplotlib_formats('svg', 'pdf') # For export\n",
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "import zipfile\n",
        "# Github URL where the dataset is stored for this tutorial\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial10/\"\n",
        "# Files to download\n",
        "pretrained_files = [(DATASET_PATH, \"TinyImageNet.zip\"), (CHECKPOINT_PATH, \"patches.zip\")]\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(DATASET_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for dir_name, file_name in pretrained_files:\n",
        "    file_path = os.path.join(dir_name, file_name)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)\n",
        "        if file_name.endswith(\".zip\"):\n",
        "            print(\"Unzipping file...\")\n",
        "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(file_path.rsplit(\"/\",1)[0])"
      ],
      "metadata": {
        "id": "y0gqYeFqvJ2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28a7fbcd-00fa-4725-ff5e-3d6b1ab44e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial10/TinyImageNet.zip...\n",
            "Unzipping file...\n",
            "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial10/patches.zip...\n",
            "Unzipping file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CNN architecture pretrained on ImageNet\n",
        "os.environ[\"TORCH_HOME\"] = CHECKPOINT_PATH\n",
        "pretrained_model = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
        "pretrained_model = pretrained_model.to(device)\n",
        "\n",
        "# No gradients needed for the network\n",
        "pretrained_model.eval()\n",
        "for p in pretrained_model.parameters():\n",
        "    p.requires_grad = False"
      ],
      "metadata": {
        "id": "uvSYUF7ZvUZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "542dd617-0eb0-4a30-b05f-32be82c06909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to ../saved_models/tutorial10/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:01<00:00, 76.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean and Std from ImageNet\n",
        "NORM_MEAN = np.array([0.485, 0.456, 0.406])\n",
        "NORM_STD = np.array([0.229, 0.224, 0.225])\n",
        "# No resizing and center crop necessary as images are already preprocessed.\n",
        "plain_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=NORM_MEAN,\n",
        "                         std=NORM_STD)\n",
        "])\n",
        "\n",
        "# Load dataset and create data loader\n",
        "imagenet_path = os.path.join(DATASET_PATH, \"TinyImageNet/\")\n",
        "assert os.path.isdir(imagenet_path), f\"Could not find the ImageNet dataset at expected path \\\"{imagenet_path}\\\". \" + \\\n",
        "                                     f\"Please make sure to have downloaded the ImageNet dataset here, or change the {DATASET_PATH=} variable.\"\n",
        "dataset = torchvision.datasets.ImageFolder(root=imagenet_path, transform=plain_transforms)\n",
        "data_loader = data.DataLoader(dataset, batch_size=32, shuffle=False, drop_last=False, num_workers=8)\n",
        "\n",
        "# Load label names to interpret the label numbers 0 to 999\n",
        "with open(os.path.join(imagenet_path, \"label_list.json\"), \"r\") as f:\n",
        "    label_names = json.load(f)\n",
        "\n",
        "def get_label_index(lab_str):\n",
        "    assert lab_str in label_names, f\"Label \\\"{lab_str}\\\" not found. Check the spelling of the class.\"\n",
        "    return label_names.index(lab_str)"
      ],
      "metadata": {
        "id": "WrPfDUhkvf3F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "740345bc-8194-4792-dc4b-c08e6c661311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TENSOR_MEANS, TENSOR_STD = torch.FloatTensor(NORM_MEAN)[:,None,None], torch.FloatTensor(NORM_STD)[:,None,None]\n",
        "def patch_forward(patch):\n",
        "    # Map patch values from [-infty,infty] to ImageNet min and max\n",
        "    patch = (torch.tanh(patch) + 1 - 2 * TENSOR_MEANS) / (2 * TENSOR_STD)\n",
        "    return patch"
      ],
      "metadata": {
        "id": "NyUWZz-qxFaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def place_single_large_patch(img, patch):\n",
        "    \"\"\"Place one single large patch randomly on the image\"\"\"\n",
        "    for i in range(img.shape[0]):  # For each image in the batch\n",
        "        h_offset = np.random.randint(0, img.shape[2] - patch.shape[1] - 1)\n",
        "        w_offset = np.random.randint(0, img.shape[3] - patch.shape[2] - 1)\n",
        "        img[i, :, h_offset:h_offset + patch.shape[1], w_offset:w_offset + patch.shape[2]] = patch_forward(patch)\n",
        "    return img\n"
      ],
      "metadata": {
        "id": "t_ay3oQ74_xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def place_multiple_small_patches(img, patch, num_patches=3):\n",
        "    \"\"\"Place multiple small patches randomly on the image\"\"\"\n",
        "    for i in range(img.shape[0]):  # For each image in the batch\n",
        "        for _ in range(num_patches):  # Place multiple patches\n",
        "            h_offset = np.random.randint(0, img.shape[2] - patch.shape[1] - 1)\n",
        "            w_offset = np.random.randint(0, img.shape[3] - patch.shape[2] - 1)\n",
        "            img[i, :, h_offset:h_offset + patch.shape[1], w_offset:w_offset + patch.shape[2]] = patch_forward(patch)\n",
        "    return img\n"
      ],
      "metadata": {
        "id": "iKJoFZ-vxID8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_patch(model, patch, val_loader, target_class):\n",
        "    model.eval()\n",
        "    tp, tp_5, counter = 0., 0., 0.\n",
        "    with torch.no_grad():\n",
        "        for img, img_labels in tqdm(val_loader, desc=\"Validating...\", leave=False):\n",
        "            # For stability, place the patch at 4 random locations per image, and average the performance\n",
        "            for _ in range(4):\n",
        "                patch_img = place_multiple_small_patches(img, patch)\n",
        "                patch_img = patch_img.to(device)\n",
        "                img_labels = img_labels.to(device)\n",
        "                pred = model(patch_img)\n",
        "                # In the accuracy calculation, we need to exclude the images that are of our target class\n",
        "                # as we would not \"fool\" the model into predicting those\n",
        "                tp += torch.logical_and(pred.argmax(dim=-1) == target_class, img_labels != target_class).sum()\n",
        "                tp_5 += torch.logical_and((pred.topk(5, dim=-1)[1] == target_class).any(dim=-1), img_labels != target_class).sum()\n",
        "                counter += (img_labels != target_class).sum()\n",
        "    acc = tp/counter\n",
        "    top5 = tp_5/counter\n",
        "    return acc, top5"
      ],
      "metadata": {
        "id": "mrexsggQxPni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def patch_attack_multiple(model, target_class, patch_size=64, num_patches=3, num_epochs=5):\n",
        "    # Split the dataset into train and validation sets\n",
        "    train_set, val_set = torch.utils.data.random_split(dataset, [4500, 500])\n",
        "    train_loader = data.DataLoader(train_set, batch_size=32, shuffle=True, drop_last=True, num_workers=8)\n",
        "    val_loader = data.DataLoader(val_set, batch_size=32, shuffle=False, drop_last=False, num_workers=4)\n",
        "\n",
        "    # Create patch parameter and optimizer\n",
        "    if not isinstance(patch_size, tuple):\n",
        "        patch_size = (patch_size, patch_size)\n",
        "    patch = nn.Parameter(torch.zeros(3, patch_size[0], patch_size[1]), requires_grad=True)\n",
        "    optimizer = torch.optim.SGD([patch], lr=1e-1, momentum=0.8)\n",
        "    loss_module = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        t = tqdm(train_loader, leave=False)\n",
        "        for img, _ in t:\n",
        "            img = place_multiple_small_patches(img, patch, num_patches=num_patches)  # Apply multiple small patches\n",
        "            img = img.to(device)\n",
        "            pred = model(img)\n",
        "            labels = torch.zeros(img.shape[0], device=pred.device, dtype=torch.long).fill_(target_class)\n",
        "            loss = loss_module(pred, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            optimizer.step()\n",
        "            t.set_description(f\"Epoch {epoch}, Loss: {loss.item():4.2f}\")\n",
        "\n",
        "    # Final validation\n",
        "    acc, top5 = eval_patch(model, patch, val_loader, target_class)\n",
        "\n",
        "    return patch.data, {\"acc\": acc.item(), \"top5\": top5.item()}\n"
      ],
      "metadata": {
        "id": "NAAjz-aTxZkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def patch_attack_single(model, target_class, patch_size=64, num_epochs=5):\n",
        "    \"\"\"Attack with a single large patch\"\"\"\n",
        "    train_set, val_set = torch.utils.data.random_split(dataset, [4500, 500])\n",
        "    train_loader = data.DataLoader(train_set, batch_size=32, shuffle=True, drop_last=True, num_workers=8)\n",
        "    val_loader = data.DataLoader(val_set, batch_size=32, shuffle=False, drop_last=False, num_workers=4)\n",
        "\n",
        "    # Create patch parameter and optimizer\n",
        "    patch = nn.Parameter(torch.zeros(3, patch_size, patch_size), requires_grad=True)\n",
        "    optimizer = torch.optim.SGD([patch], lr=1e-1, momentum=0.8)\n",
        "    loss_module = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        t = tqdm(train_loader, leave=False)\n",
        "        for img, _ in t:\n",
        "            img = place_single_large_patch(img, patch)  # Apply the large patch\n",
        "            img = img.to(device)\n",
        "            pred = model(img)\n",
        "            labels = torch.zeros(img.shape[0], device=pred.device, dtype=torch.long).fill_(target_class)\n",
        "            loss = loss_module(pred, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            optimizer.step()\n",
        "            t.set_description(f\"Epoch {epoch}, Loss: {loss.item():4.2f}\")\n",
        "\n",
        "    # Final validation\n",
        "    acc, top5 = eval_patch(model, patch, val_loader, target_class)\n",
        "\n",
        "    return patch.data, {\"acc\": acc.item(), \"top5\": top5.item()}\n"
      ],
      "metadata": {
        "id": "VXGA7ywG5Eur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load evaluation results of the pretrained patches\n",
        "json_results_file = os.path.join(CHECKPOINT_PATH, \"patch_results.json\")\n",
        "json_results = {}\n",
        "if os.path.isfile(json_results_file):\n",
        "    with open(json_results_file, \"r\") as f:\n",
        "        json_results = json.load(f)\n",
        "\n",
        "# If you train new patches, you can save the results via calling this function\n",
        "def save_results(patch_dict):\n",
        "    result_dict = {cname: {psize: [t.item() if isinstance(t, torch.Tensor) else t\n",
        "                                   for t in patch_dict[cname][psize][\"results\"]]\n",
        "                           for psize in patch_dict[cname]}\n",
        "                   for cname in patch_dict}\n",
        "    with open(os.path.join(CHECKPOINT_PATH, \"patch_results.json\"), \"w\") as f:\n",
        "        json.dump(result_dict, f, indent=4)"
      ],
      "metadata": {
        "id": "anX-jjbSxefq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_patches(class_names, patch_sizes):\n",
        "    result_dict = dict()\n",
        "\n",
        "    # Loop over all classes and patch sizes\n",
        "    for name in class_names:\n",
        "        result_dict[name] = dict()\n",
        "        for patch_size in patch_sizes:\n",
        "            c = label_names.index(name)\n",
        "            file_name = os.path.join(CHECKPOINT_PATH, f\"{name}_{patch_size}_patch.pt\")\n",
        "            # Load patch if pretrained file exists, otherwise start training\n",
        "            if not os.path.isfile(file_name):\n",
        "                patch, val_results = patch_attack_multiple(pretrained_model, target_class=c, patch_size=patch_size, num_epochs=5)\n",
        "                print(f\"Validation results for {name} and {patch_size}:\", val_results)\n",
        "                torch.save(patch, file_name)\n",
        "            else:\n",
        "                patch = torch.load(file_name)\n",
        "            # Load evaluation results if exist, otherwise manually evaluate the patch\n",
        "            if name in json_results:\n",
        "                results = json_results[name][str(patch_size)]\n",
        "            else:\n",
        "                results = eval_patch(pretrained_model, patch, data_loader, target_class=c)\n",
        "\n",
        "            # Store results and the patches in a dict for better access\n",
        "            result_dict[name][patch_size] = {\n",
        "                \"results\": results,\n",
        "                \"patch\": patch\n",
        "            }\n",
        "\n",
        "    return result_dict"
      ],
      "metadata": {
        "id": "umdnJeDaxlQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate one large patch\n",
        "patch_size_large = 64\n",
        "large_patch, results_large = patch_attack_single(pretrained_model, target_class=get_label_index('manhole cover'), patch_size=patch_size_large, num_epochs=5)\n",
        "\n",
        "# Evaluate multiple small patches\n",
        "patch_size_small = 32\n",
        "num_patches = 3  # Place the small patch multiple times\n",
        "small_patch, results_small = patch_attack_multiple(pretrained_model, target_class=get_label_index('manhole cover'), patch_size=patch_size_small, num_patches=num_patches, num_epochs=5)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Results with one large patch: {results_large}\")\n",
        "print(f\"Results with multiple small patches: {results_small}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "bda66464bcb344f09533a19a6eec2201",
            "b0a54732562a4dc088be68f00876bc53",
            "4e4081e808534d46a97701da9f272025",
            "af8e0df5f86f44fbbf2ab13e5ae19245",
            "2082e738c0c04e29a5e46488aa418267",
            "221ceca54649428f8f8a96b96e971550",
            "cda1c78ae8b24849bb01e2520c8ec612",
            "5ea5bb22f13d49fca038318bc3d9e760",
            "3ce50966e61c4d46934346d679a8b05c",
            "805c29166cbd4c9289b80b4f87d1819a",
            "988c3cc96f9443f78a23c5831bf6307b"
          ]
        },
        "id": "rdNducYnblQP",
        "outputId": "a8ecfc89-c75a-42fb-80af-fde5d60c0c4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/140 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bda66464bcb344f09533a19a6eec2201"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['manhole cover', 'necklace','pineapple']\n",
        "patch_sizes = [64,32]\n",
        "\n",
        "patch_dict = get_patches(class_names, patch_sizes)\n",
        "save_results(patch_dict) # Uncomment if you add new class names and want to save the new results"
      ],
      "metadata": {
        "id": "XQimZQBmxpCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the generated patches"
      ],
      "metadata": {
        "id": "xFXX6oLt59M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_patches():\n",
        "    fig, ax = plt.subplots(len(patch_sizes), len(class_names), figsize=(len(class_names)*2.2, len(patch_sizes)*2.2))\n",
        "    for c_idx, cname in enumerate(class_names):\n",
        "        for p_idx, psize in enumerate(patch_sizes):\n",
        "            patch = patch_dict[cname][psize][\"patch\"]\n",
        "            patch = (torch.tanh(patch) + 1) / 2 # Parameter to pixel values\n",
        "            patch = patch.cpu().permute(1, 2, 0).numpy()\n",
        "            patch = np.clip(patch, a_min=0.0, a_max=1.0)\n",
        "            ax[p_idx][c_idx].imshow(patch)\n",
        "            ax[p_idx][c_idx].set_title(f\"{cname}, size {psize}\")\n",
        "            ax[p_idx][c_idx].axis('off')\n",
        "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "    plt.show()\n",
        "show_patches()"
      ],
      "metadata": {
        "id": "alH6c6ldxxiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_prediction(img, label, pred, K=5, adv_img=None, noise=None):\n",
        "\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        # Tensor image to numpy\n",
        "        img = img.cpu().permute(1, 2, 0).numpy()\n",
        "        img = (img * NORM_STD[None,None]) + NORM_MEAN[None,None]\n",
        "        img = np.clip(img, a_min=0.0, a_max=1.0)\n",
        "        label = label.item()\n",
        "\n",
        "    # Plot on the left the image with the true label as title.\n",
        "    # On the right, have a horizontal bar plot with the top k predictions including probabilities\n",
        "    if noise is None or adv_img is None:\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(10,2), gridspec_kw={'width_ratios': [1, 1]})\n",
        "    else:\n",
        "        fig, ax = plt.subplots(1, 5, figsize=(12,2), gridspec_kw={'width_ratios': [1, 1, 1, 1, 2]})\n",
        "\n",
        "    ax[0].imshow(img)\n",
        "    ax[0].set_title(label_names[label])\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    if adv_img is not None and noise is not None:\n",
        "        # Visualize adversarial images\n",
        "        adv_img = adv_img.cpu().permute(1, 2, 0).numpy()\n",
        "        adv_img = (adv_img * NORM_STD[None,None]) + NORM_MEAN[None,None]\n",
        "        adv_img = np.clip(adv_img, a_min=0.0, a_max=1.0)\n",
        "        ax[1].imshow(adv_img)\n",
        "        ax[1].set_title('Adversarial')\n",
        "        ax[1].axis('off')\n",
        "        # Visualize noise\n",
        "        noise = noise.cpu().permute(1, 2, 0).numpy()\n",
        "        noise = noise * 0.5 + 0.5 # Scale between 0 to 1\n",
        "        ax[2].imshow(noise)\n",
        "        ax[2].set_title('Noise')\n",
        "        ax[2].axis('off')\n",
        "        # buffer\n",
        "        ax[3].axis('off')\n",
        "\n",
        "    if abs(pred.sum().item() - 1.0) > 1e-4:\n",
        "        pred = torch.softmax(pred, dim=-1)\n",
        "    topk_vals, topk_idx = pred.topk(K, dim=-1)\n",
        "    topk_vals, topk_idx = topk_vals.cpu().numpy(), topk_idx.cpu().numpy()\n",
        "    ax[-1].barh(np.arange(K), topk_vals*100.0, align='center', color=[\"C0\" if topk_idx[i]!=label else \"C2\" for i in range(K)])\n",
        "    ax[-1].set_yticks(np.arange(K))\n",
        "    ax[-1].set_yticklabels([label_names[c] for c in topk_idx])\n",
        "    ax[-1].invert_yaxis()\n",
        "    ax[-1].set_xlabel('Confidence')\n",
        "    ax[-1].set_title('Predictions')\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "mKML7xkoOWuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exmp_batch, label_batch = next(iter(data_loader))\n",
        "with torch.no_grad():\n",
        "    preds = pretrained_model(exmp_batch.to(device))\n",
        "for i in range(1,17,5):\n",
        "    show_prediction(exmp_batch[i], label_batch[i], preds[i])"
      ],
      "metadata": {
        "id": "HHWY7FyXORZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_patch_attack_multiple(patch):\n",
        "    patch_batch = exmp_batch.clone()\n",
        "    patch_batch = place_multiple_small_patches(patch_batch, patch)\n",
        "    with torch.no_grad():\n",
        "        patch_preds = pretrained_model(patch_batch.to(device))\n",
        "    for i in range(1,17,5):\n",
        "        show_prediction(patch_batch[i], label_batch[i], patch_preds[i])"
      ],
      "metadata": {
        "id": "g15f0Bf9xx0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uzJ5Whi0N0Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perform_patch_attack_multiple(patch_dict['manhole cover'][32]['patch'])"
      ],
      "metadata": {
        "id": "O0CnT4KJx2bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perform_patch_attack_single(patch_dict['necklace'][64]['patch'])"
      ],
      "metadata": {
        "id": "EX5l5IWf5hHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perform_patch_attack_multiple(patch_dict['pineapple'][32]['patch'])"
      ],
      "metadata": {
        "id": "gKBFBGT05hnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_patches_comparison(large_patch, small_patch):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    # Large patch visualization\n",
        "    large_patch_vis = (torch.tanh(large_patch) + 1) / 2  # Parameter to pixel values\n",
        "    large_patch_vis = large_patch_vis.cpu().permute(1, 2, 0).numpy()\n",
        "    large_patch_vis = np.clip(large_patch_vis, a_min=0.0, a_max=1.0)\n",
        "    ax[0].imshow(large_patch_vis)\n",
        "    ax[0].set_title(f\"One Large Patch\")\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    # Small patch visualization\n",
        "    small_patch_vis = (torch.tanh(small_patch) + 1) / 2  # Parameter to pixel values\n",
        "    small_patch_vis = small_patch_vis.cpu().permute(1, 2, 0).numpy()\n",
        "    small_patch_vis = np.clip(small_patch_vis, a_min=0.0, a_max=1.0)\n",
        "    ax[1].imshow(small_patch_vis)\n",
        "    ax[1].set_title(f\"Multiple Small Patches\")\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "show_patches_comparison(large_patch, small_patches)\n"
      ],
      "metadata": {
        "id": "1blc63t0bpfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference:\n",
        "\n",
        "\n",
        "1.   https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial10/Adversarial_Attacks.ipynb#scrollTo=EvWZTPmL1sGP\n",
        "2.   Perplexity AI\n",
        "\n"
      ],
      "metadata": {
        "id": "RdKnH6tDyjXi"
      }
    }
  ]
}